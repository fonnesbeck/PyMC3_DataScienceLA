{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced MCMC methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, some more involved algorithms that improve on basic Metropolis-Hastings are introduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hamiltonian Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While flexible and easy to implement, Metropolis-Hastings sampling is a random walk\n",
    "sampler that might not be statistically efficient for many models. In\n",
    "this context, and when sampling from continuous variables, Hamiltonian (or Hybrid) Monte\n",
    "Carlo (HMC) can prove to be a powerful tool. It avoids\n",
    "random walk behavior by simulating a physical system governed by\n",
    "Hamiltonian dynamics, potentially avoiding tricky conditional\n",
    "distributions in the process.\n",
    "\n",
    "![hmc comparison](images/hmc.png)\n",
    "\n",
    "In HMC, model samples are obtained by simulating a physical system,\n",
    "where particles move about a high-dimensional landscape, subject to\n",
    "potential and kinetic energies. Adapting the notation from [Neal (1993)](http://www.cs.toronto.edu/~radford/review.abstract.html),\n",
    "particles are characterized by a position vector or state\n",
    "$s \\in \\mathcal{R}^D$ and velocity vector $\\phi \\in \\mathcal{R}^D$. The\n",
    "combined state of a particle is denoted as $\\chi=(s,\\phi)$. The\n",
    "Hamiltonian is then defined as the sum of potential energy $E(s)$ and kinetic energy\n",
    "$K(\\phi)$, as follows:\n",
    "\n",
    "$$\\mathcal{H}(s,\\phi) = E(s) + K(\\phi)\n",
    "= E(s) + \\frac{1}{2} \\sum_i \\phi_i^2$$\n",
    "\n",
    "Instead of sampling $p(s)$ directly, HMC operates by sampling from the\n",
    "canonical distribution\n",
    "$p(s,\\phi) = \\frac{1}{Z} \\exp(-\\mathcal{H}(s,\\phi))=p(s)p(\\phi)$.\n",
    "Because the two variables are independent, marginalizing over $\\phi$ is\n",
    "trivial and recovers the original distribution of interest.\n",
    "\n",
    "**Hamiltonian Dynamics**\n",
    "\n",
    "State $s$ and velocity $\\phi$ are modified such that\n",
    "$\\mathcal{H}(s,\\phi)$ remains constant throughout the simulation. The\n",
    "differential equations are given by:\n",
    "\n",
    "$$\\begin{aligned}\\frac{ds_i}{dt} &= \\frac{\\partial \\mathcal{H}}{\\partial \\phi_i} = \\phi_i \\\\\n",
    "\\frac{d\\phi_i}{dt} &= - \\frac{\\partial \\mathcal{H}}{\\partial s_i}\n",
    "= - \\frac{\\partial E}{\\partial s_i}\n",
    "\\end{aligned}$$\n",
    "\n",
    "As shown in [Neal (1993)](http://www.cs.toronto.edu/~radford/review.abstract.html), \n",
    "the above transformation preserves volume and is\n",
    "reversible. The above dynamics can thus be used as transition operators\n",
    "of a Markov chain and will leave $p(s,\\phi)$ invariant. That chain by\n",
    "itself is not ergodic however, since simulating the dynamics maintains a\n",
    "fixed Hamiltonian $\\mathcal{H}(s,\\phi)$. HMC thus alternates Hamiltonian\n",
    "dynamic steps, with Gibbs sampling of the velocity. Because $p(s)$ and\n",
    "$p(\\phi)$ are independent, sampling $\\phi_{new} \\sim p(\\phi|s)$ is\n",
    "trivial since $p(\\phi|s)=p(\\phi)$, where $p(\\phi)$ is often taken to be\n",
    "the univariate Gaussian.\n",
    "\n",
    "**The Leap-Frog Algorithm**\n",
    "\n",
    "In practice, we cannot simulate Hamiltonian dynamics exactly because of\n",
    "the problem of time discretization. There are several ways one can do\n",
    "this. To maintain invariance of the Markov chain however, care must be\n",
    "taken to preserve the properties of *volume conservation* and *time\n",
    "reversibility*. The **leap-frog algorithm** maintains these properties\n",
    "and operates in 3 steps:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\phi_i(t + \\epsilon/2) &= \\phi_i(t) - \\frac{\\epsilon}{2} \\frac{\\partial{}}{\\partial s_i} E(s(t)) \\\\\n",
    "s_i(t + \\epsilon) &= s_i(t) + \\epsilon \\phi_i(t + \\epsilon/2) \\\\\n",
    "\\phi_i(t + \\epsilon) &= \\phi_i(t + \\epsilon/2) - \\frac{\\epsilon}{2} \\frac{\\partial{}}{\\partial s_i} E(s(t + \\epsilon)) \n",
    "\\end{aligned}$$\n",
    "\n",
    "We thus perform a half-step update of the velocity at time\n",
    "$t+\\epsilon/2$, which is then used to compute $s(t + \\epsilon)$ and\n",
    "$\\phi(t + \\epsilon)$.\n",
    "\n",
    "**Accept / Reject**\n",
    "\n",
    "In practice, using finite stepsizes $\\epsilon$ will not preserve\n",
    "$\\mathcal{H}(s,\\phi)$ exactly and will introduce bias in the simulation.\n",
    "Also, rounding errors due to the use of floating point numbers means\n",
    "that the above transformation will not be perfectly reversible.\n",
    "\n",
    "HMC cancels these effects **exactly** by adding a Metropolis\n",
    "accept/reject stage, after $n$ leapfrog steps. The new state\n",
    "$\\chi' = (s',\\phi')$ is accepted with probability $p_{acc}(\\chi,\\chi')$,\n",
    "defined as:\n",
    "\n",
    "$$p_{acc}(\\chi,\\chi') = min \\left( 1, \\frac{\\exp(-\\mathcal{H}(s',\\phi')}{\\exp(-\\mathcal{H}(s,\\phi)} \\right)$$\n",
    "\n",
    "**HMC Algorithm**\n",
    "\n",
    "We obtain a new HMC sample as follows:\n",
    "\n",
    "1.  sample a new velocity from a univariate Gaussian distribution\n",
    "2.  perform $n$ leapfrog steps to obtain the new state $\\chi'$\n",
    "3.  perform accept/reject move of $\\chi'$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No U-Turn Sampling\n",
    "\n",
    "The major drawback of the HMC algorithm is the extensive tuning required to make it sample efficiency. There are a handful of parameters that require specification by the user:\n",
    "\n",
    "- the scaling of the momentum distribution\n",
    "- the step size forthe leapfrog algorithm\n",
    "- the number of steps to be taken for the leapfrog algorithm\n",
    "\n",
    "When these parameters are poorly-chosen, the HMC algorithm can suffer severe losses in efficiency. For example, if we take steps that are too short, the simulation becomes a random walk, while steps that are too long end up retracing paths already taken.\n",
    "\n",
    "An efficient MCMC algorithm seeks to optimize mixing, while maintaining detailed balance. While HMC can be tuned on-the-fly, it requires costly burn-in runs to do so.\n",
    "\n",
    "![nuts](images/nuts.png)\n",
    "\n",
    "The No U-turn Sampling (NUTS) algorithm automatically tunes the step size and step number parameters, without any intervention from the user. To do so, NUTS constructs a binary tree of leapfrog steps by repeated doubling. When the trajectory of steps creates an angle of more than 90 degrees (*i.e.* a u-turn), the doubling stops, and a point is proposed.\n",
    "\n",
    "![binary doubling](images/binary_doubling.png)\n",
    "\n",
    "NUTS provides the efficiency of gradient-based MCMC sampling without extensive user intervention required to tune Hamiltonian Monte Carlo. As the result, NUTS is the default sampling algorithm for continuous variables in PyMC3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. [Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., and Rubin, D. B. (2013)](http://www.stat.columbia.edu/~gelman/book/). Bayesian Data Analysis. Chapman &Hall/CRC Press, London, third edition.\n",
    "2. [Geyer, C. (2013)](http://www.mcmchandbook.net/HandbookChapter1.pdf) Introduction to Markov Chain Monte Carlo. In *Handbook of Markov Chain Monte Carlo*, S. Brooks, A. Gelman, G. Jones, X.L. Meng, eds. CRC Press.\n",
    "3. [Neal, R.M. (1993)](http://www.cs.toronto.edu/~radford/review.abstract.html) Probabilistic Inference Using Markov Chain Monte Carlo Methods, Technical Report CRG-TR-93-1, Dept. of Computer Science, University of Toronto, 144 pages.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
